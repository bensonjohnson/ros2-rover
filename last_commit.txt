commit d30a737f7443e799b320fd0199a6989ac8f021d6
Author: bensonjohnson <benson.johnson@me.com>
Date:   Sat Dec 6 11:19:27 2025 -0700

    feat: Enhance OccupancyGridEncoder for 4-channel 128x128 input with spatial attention, preserving spatial structure, and update SAC models and RKNN conversion.

diff --git a/.claude/settings.local.json b/.claude/settings.local.json
index bf1a6a5..a201253 100644
--- a/.claude/settings.local.json
+++ b/.claude/settings.local.json
@@ -17,7 +17,8 @@
       "Bash(git push)",
       "Bash(pgrep:*)",
       "Bash(find:*)",
-      "Bash(grep:*)"
+      "Bash(grep:*)",
+      "Bash(cat:*)"
     ],
     "deny": [],
     "ask": []
diff --git a/diff_cached.txt b/diff_cached.txt
new file mode 100644
index 0000000..e69de29
diff --git a/remote_training_server/export_to_rknn.py b/remote_training_server/export_to_rknn.py
index 1425c54..1843574 100644
--- a/remote_training_server/export_to_rknn.py
+++ b/remote_training_server/export_to_rknn.py
@@ -58,8 +58,10 @@ class RKNNConverter:
         # Configure RKNN
         print("Configuring RKNN...")
         ret = self.rknn.config(
-            mean_values=[[0, 0, 0], [0], [0]*6],  # RGB: [0, 255]->[0, 1], Depth: [0, 1]->[0, 1], Proprio: No norm
-            std_values=[[255, 255, 255], [1], [1]*6],
+            # Input 0: Grid (4, 128, 128) float32, already normalized to [0, 1]
+            # Input 1: Proprio (10,) float32, various ranges
+            mean_values=[[0, 0, 0, 0], [0]*10],  # Grid: no normalization needed, Proprio: no normalization
+            std_values=[[1, 1, 1, 1], [1]*10],   # Grid: already [0,1], Proprio: as-is
             target_platform=target_platform,
             quantized_dtype='asymmetric_quantized-8' if quantize else 'float16',
             quantized_algorithm='normal',
@@ -108,8 +110,10 @@ class RKNNConverter:
     def _prepare_calibration_dataset(self, num_samples: int = 100):
         """Prepare calibration dataset for quantization.
 
-        The calibration dataset should contain representative RGB-D samples
+        The calibration dataset should contain representative multi-channel grid samples
         collected from the rover during operation.
+
+        Expected format: grid (4, 128, 128) float32 + proprio (10,) float32
         """
         if not self.calibration_data_dir or not os.path.exists(self.calibration_data_dir):
             print(f"WARNING: Calibration data directory not found: {self.calibration_data_dir}")
@@ -117,7 +121,7 @@ class RKNNConverter:
 
         print(f"Loading calibration data from {self.calibration_data_dir}...")
 
-        # Look for .npz files saved by the observation node
+        # Look for .npz files saved by the SAC episode runner
         calibration_files = list(Path(self.calibration_data_dir).glob('*.npz'))
 
         if not calibration_files:
@@ -128,35 +132,20 @@ class RKNNConverter:
         dataset = []
         for i, file_path in enumerate(calibration_files[:num_samples]):
             try:
-                # Try to load keys directly (if saved from ppo_episode_runner buffer)
                 data = np.load(file_path)
-                if 'rgb' in data and 'depth' in data:
-                    rgb = data['rgb']
-                    depth = data['depth']
-                    # Proprio might be there or not
-                    if 'proprio' in data:
-                        proprio = data['proprio']
+
+                # New format: grid (4, 128, 128) + proprio (10,)
+                if 'grid' in data and 'proprio' in data:
+                    grid = data['grid']
+                    proprio = data['proprio']
+
+                    # Validate shapes
+                    if grid.shape == (4, 128, 128) and proprio.shape == (10,):
+                        dataset.append({'grid': grid, 'proprio': proprio})
                     else:
-                        proprio = np.zeros(6, dtype=np.float32)
-                    dataset.append({'rgb': rgb, 'depth': depth, 'proprio': proprio})
-                    continue
-
-                # Fallback to 'observation' key (legacy/gym format)
-                if 'observation' in data:
-                    obs = data['observation']
-                    if obs.shape[0] >= 3:  # Has RGB (3 channels) + Depth (1 channel)
-                        # Extract RGB (first 3 channels) and depth (channel index 3)
-                        # Assuming format is (C, H, W)
-                        rgb = obs[:3].transpose(1, 2, 0)  # (H, W, 3)
-                        depth = obs[3]  # Depth channel
-                        proprio = np.zeros(6, dtype=np.float32) # Dummy proprio
-                        dataset.append({'rgb': rgb, 'depth': depth, 'proprio': proprio})
-                    elif obs.shape[0] >= 2:  # Has at least occupancy + depth (fallback)
-                        # Create dummy RGB for now (you'd extract real RGB from your data)
-                        rgb = np.random.randint(0, 255, (240, 424, 3), dtype=np.uint8)
-                        depth = obs[1]  # Depth channel
-                        proprio = np.zeros(6, dtype=np.float32) # Dummy proprio
-                        dataset.append({'rgb': rgb, 'depth': depth, 'proprio': proprio})
+                        print(f"WARNING: Unexpected shapes in {file_path}: grid={grid.shape}, proprio={proprio.shape}")
+                else:
+                    print(f"WARNING: Missing 'grid' or 'proprio' in {file_path}")
 
             except Exception as exc:
                 print(f"Failed to load {file_path}: {exc}")
@@ -164,15 +153,18 @@ class RKNNConverter:
 
         if not dataset:
             print("WARNING: No valid calibration samples loaded")
+            print("TIP: Run the rover with SAC to collect calibration data first")
             return None
 
         print(f"Loaded {len(dataset)} calibration samples")
 
         # Convert to RKNN dataset format
-        # Note: This is a simplified version - adjust based on your model inputs
         def data_generator():
             for sample in dataset:
-                yield [sample['rgb'], sample['depth'], sample['proprio']]
+                # RKNN expects: [input0, input1, ...]
+                # Input 0: grid (4, 128, 128) float32
+                # Input 1: proprio (10,) float32
+                yield [sample['grid'], sample['proprio']]
 
         return data_generator
 
@@ -182,8 +174,13 @@ class RKNNConverter:
         sdk_version = self.rknn.get_sdk_version()
         print(f"RKNN SDK Version: {sdk_version}")
 
-    def test_inference(self, test_rgb: np.ndarray, test_depth: np.ndarray):
-        """Test inference on sample data."""
+    def test_inference(self, test_grid: np.ndarray, test_proprio: np.ndarray):
+        """Test inference on sample data.
+
+        Args:
+            test_grid: (4, 128, 128) float32 multi-channel occupancy grid
+            test_proprio: (10,) float32 proprioception vector
+        """
         print("Testing RKNN inference...")
 
         # Initialize runtime on simulator (for testing on x86)
@@ -192,8 +189,12 @@ class RKNNConverter:
             print(f"RKNN init_runtime failed: {ret}")
             return
 
+        # Validate inputs
+        assert test_grid.shape == (4, 128, 128), f"Expected grid shape (4, 128, 128), got {test_grid.shape}"
+        assert test_proprio.shape == (10,), f"Expected proprio shape (10,), got {test_proprio.shape}"
+
         # Run inference
-        outputs = self.rknn.inference(inputs=[test_rgb, test_depth])
+        outputs = self.rknn.inference(inputs=[test_grid, test_proprio])
         print(f"Inference output shape: {[o.shape for o in outputs]}")
         print(f"Action: {outputs[0]}")
 
diff --git a/remote_training_server/model_architectures.py b/remote_training_server/model_architectures.py
index 0f25ee5..6929832 100644
--- a/remote_training_server/model_architectures.py
+++ b/remote_training_server/model_architectures.py
@@ -10,40 +10,73 @@ import torch.nn as nn
 from typing import Tuple
 
 
-class OccupancyGridEncoder(nn.Module):
-    """Vision encoder for Top-Down Occupancy Grid.
+class SpatialAttention(nn.Module):
+    """
+    Learns to focus on important spatial regions (gaps, obstacles).
+    Applies channel-wise attention to emphasize relevant features.
+    """
+
+    def __init__(self, channels):
+        super().__init__()
+        self.conv = nn.Conv2d(channels, 1, kernel_size=1)
+
+    def forward(self, x):
+        attention = torch.sigmoid(self.conv(x))  # (B, 1, H, W)
+        return x * attention  # Element-wise weighting
 
-    Takes 64x64 Occupancy Grid (1 channel) and encodes it.
+
+class OccupancyGridEncoder(nn.Module):
+    """
+    Encoder for 4-channel 128×128 occupancy grid.
+    Preserves spatial structure without global pooling.
+
+    Input channels:
+    - 0: Distance to nearest obstacle [0.0, 1.0]
+    - 1: Exploration history [0.0, 1.0]
+    - 2: Obstacle confidence [0.0, 1.0]
+    - 3: Terrain height [0.0, 1.0]
     """
 
-    def __init__(self, input_channels: int = 1):
+    def __init__(self, input_channels: int = 4):
         super().__init__()
 
+        # Input: (B, 4, 128, 128)
         self.conv = nn.Sequential(
-            # Input: (B, 1, 64, 64)
-            nn.Conv2d(input_channels, 32, kernel_size=3, stride=2, padding=1), # -> (32, 32, 32)
+            # Stage 1: 128 → 64
+            nn.Conv2d(input_channels, 32, kernel_size=3, stride=2, padding=1),
+            nn.ReLU(inplace=True),
+
+            # Stage 2: 64 → 32
+            nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1),
             nn.ReLU(inplace=True),
-            nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1), # -> (64, 16, 16)
+
+            # Stage 3: 32 → 16 (with spatial attention)
+            nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1),
             nn.ReLU(inplace=True),
-            nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1), # -> (128, 8, 8)
+            SpatialAttention(128),  # Focus on relevant regions
+
+            # Stage 4: 16 → 8
+            nn.Conv2d(128, 256, kernel_size=3, stride=2, padding=1),
             nn.ReLU(inplace=True),
-            nn.Conv2d(128, 256, kernel_size=3, stride=2, padding=1), # -> (256, 4, 4)
+
+            # Stage 5: 8 → 4
+            nn.Conv2d(256, 256, kernel_size=3, stride=2, padding=1),
             nn.ReLU(inplace=True),
         )
 
-        self.pool = nn.AdaptiveAvgPool2d(1)
-        self.output_dim = 256
+        # NO global pooling! Flatten spatial features
+        self.output_dim = 256 * 4 * 4  # 4096
 
     def forward(self, grid: torch.Tensor) -> torch.Tensor:
         """
         Args:
-            grid: (B, 1, 64, 64) normalized to [0, 1]
+            grid: (B, 4, 128, 128) normalized to [0, 1]
         Returns:
-            features: (B, 256)
+            features: (B, 4096) - preserves spatial info!
         """
-        features = self.conv(grid)
-        features = self.pool(features)
-        return features.view(features.size(0), -1)
+        x = self.conv(grid)  # (B, 256, 4, 4)
+        x = x.flatten(start_dim=1)  # (B, 4096) - preserves spatial structure
+        return x
 
 
 class PolicyHead(nn.Module):
@@ -157,34 +190,32 @@ class QNetwork(nn.Module):
 
     Estimates Q(s, a) - the expected return for taking action a in state s.
     """
-    def __init__(self, feature_dim: int, proprio_dim: int = 6, action_dim: int = 2, dropout: float = 0.0):
+    def __init__(self, feature_dim: int = 4096, proprio_dim: int = 10, action_dim: int = 2, dropout: float = 0.0):
         super().__init__()
 
         # Proprioception encoder
         self.proprio_encoder = nn.Sequential(
             nn.Linear(proprio_dim, 64),
             nn.ReLU(inplace=True),
-            nn.Linear(64, 64),
-            nn.ReLU(inplace=True),
         )
 
-        # Q-network: takes (features + proprio + action)
+        # Q-function: visual (4096) + proprio (64) + action (2) = 4162
         # Build with conditional dropout for DroQ
         layers = [
-            nn.Linear(feature_dim + 64 + action_dim, 256),
+            nn.Linear(feature_dim + 64 + action_dim, 512),
             nn.ReLU(inplace=True),
         ]
         if dropout > 0.0:
             layers.append(nn.Dropout(p=dropout))
 
         layers.extend([
-            nn.Linear(256, 128),
+            nn.Linear(512, 256),
             nn.ReLU(inplace=True),
         ])
         if dropout > 0.0:
             layers.append(nn.Dropout(p=dropout))
 
-        layers.append(nn.Linear(128, 1))
+        layers.append(nn.Linear(256, 1))
 
         self.q_net = nn.Sequential(*layers)
 
@@ -197,9 +228,10 @@ class QNetwork(nn.Module):
 class GaussianPolicyHead(nn.Module):
     """SAC Policy head that outputs mean and log_std for continuous actions."""
 
-    def __init__(self, feature_dim: int, proprio_dim: int, action_dim: int = 2, hidden_size: int = 256):
+    def __init__(self, feature_dim: int = 4096, proprio_dim: int = 10, action_dim: int = 2, hidden_size: int = 256):
         super().__init__()
 
+        # Proprioception encoder (unchanged)
         self.proprio_encoder = nn.Sequential(
             nn.Linear(proprio_dim, 64),
             nn.ReLU(inplace=True),
@@ -207,10 +239,11 @@ class GaussianPolicyHead(nn.Module):
             nn.ReLU(inplace=True),
         )
 
+        # Fusion: 4096 (visual) + 64 (proprio) = 4160
         self.net = nn.Sequential(
-            nn.Linear(feature_dim + 64, hidden_size),
+            nn.Linear(feature_dim + 64, 512),  # Larger first layer
             nn.ReLU(inplace=True),
-            nn.Linear(hidden_size, hidden_size),
+            nn.Linear(512, hidden_size),
             nn.ReLU(inplace=True),
         )
 
diff --git a/remote_training_server/serialization_utils.py b/remote_training_server/serialization_utils.py
index ea2c161..15ef8b2 100644
--- a/remote_training_server/serialization_utils.py
+++ b/remote_training_server/serialization_utils.py
@@ -19,7 +19,8 @@ def serialize_batch(batch: dict) -> bytes:
 
     Args:
         batch: Dictionary containing:
-            - grid: np.array of shape (N, 64, 64) uint8
+            - grid: np.array of shape (N, 4, 128, 128) float32
+                    4 channels: distance, exploration, confidence, height
             - proprio: np.array of shape (N, 10) float32
             - actions: np.array of shape (N, 2) float32
             - rewards: np.array of shape (N,) float32
diff --git a/remote_training_server/v620_sac_trainer.py b/remote_training_server/v620_sac_trainer.py
index 1081741..32857d7 100644
--- a/remote_training_server/v620_sac_trainer.py
+++ b/remote_training_server/v620_sac_trainer.py
@@ -668,12 +668,33 @@ class V620SACTrainer:
         alpha = self.log_alpha.exp().item()
 
         # --- Diagnostic Logging (TensorBoard) ---
-        if self.total_steps % 10 == 0 and last_log_prob is not None:
-            self.writer.add_scalar('train/gradient_steps', self.gradient_steps, self.total_steps)
-            self.writer.add_scalar('train/utd_ratio', self.args.utd_ratio, self.total_steps)
-            self.writer.add_scalar('train/q1_mean', last_q1.mean().item(), self.total_steps)
-            self.writer.add_scalar('train/q2_mean', last_q2.mean().item(), self.total_steps)
-            self.writer.add_scalar('train/alpha', alpha, self.total_steps)
+        if self.total_steps % 100 == 0 and last_log_prob is not None:
+            # Loss metrics
+            self.writer.add_scalar('Loss/Critic', avg_critic_loss, self.total_steps)
+            self.writer.add_scalar('Loss/Actor', avg_actor_loss, self.total_steps)
+            self.writer.add_scalar('Loss/Alpha', avg_alpha_loss, self.total_steps)
+
+            # Entropy and alpha
+            self.writer.add_scalar('Entropy/Policy', -last_log_prob.mean().item(), self.total_steps)
+            self.writer.add_scalar('Alpha/Value', alpha, self.total_steps)
+
+            # Q-values
+            self.writer.add_scalar('Q_Value/Q1', last_q1.mean().item(), self.total_steps)
+            self.writer.add_scalar('Q_Value/Q2', last_q2.mean().item(), self.total_steps)
+            self.writer.add_scalar('Q_Value/Min_Q_Pi', last_min_q_pi.mean().item() if last_min_q_pi is not None else 0.0, self.total_steps)
+
+            # Reward statistics
+            self.writer.add_scalar('Reward/Mean', batch['reward'].mean().item(), self.total_steps)
+            self.writer.add_scalar('Reward/Std', batch['reward'].std().item(), self.total_steps)
+
+            # NEW: Log observation statistics for debugging
+            self.writer.add_scalar('Observation/Grid_Mean', batch['grid'].mean().item(), self.total_steps)
+            self.writer.add_scalar('Observation/Grid_Std', batch['grid'].std().item(), self.total_steps)
+            self.writer.add_scalar('Observation/Proprio_Mean', batch['proprio'].mean().item(), self.total_steps)
+
+            # Training progress
+            self.writer.add_scalar('Training/Gradient_Steps', self.gradient_steps, self.total_steps)
+            self.writer.add_scalar('Training/UTD_Ratio', self.args.utd_ratio, self.total_steps)
 
         t3 = time.time()
 
@@ -1096,22 +1117,34 @@ class V620SACTrainer:
 if __name__ == '__main__':
     parser = argparse.ArgumentParser()
     parser.add_argument('--nats_server', type=str, default='nats://nats.gokickrocks.org:4222', help='NATS server URL')
-    parser.add_argument('--buffer_size', type=int, default=1000000) # Increased buffer size too since data is smaller
-    parser.add_argument('--batch_size', type=int, default=4096)
-    parser.add_argument('--lr', type=float, default=3e-5)
-    parser.add_argument('--gamma', type=float, default=0.97)
+
+    # Learning - Updated for improved convergence
+    parser.add_argument('--lr', type=float, default=3e-4,  # Was 3e-5 (10× too low!)
+                        help='Learning rate for Adam optimizer')
+    parser.add_argument('--batch_size', type=int, default=512,  # Was 4096
+                        help='Batch size for training (smaller = more frequent updates)')
+    parser.add_argument('--buffer_size', type=int, default=200000,  # Was 1M
+                        help='Replay buffer capacity (faster turnover)')
+
+    # SAC specific
+    parser.add_argument('--gamma', type=float, default=0.98,  # Was 0.97
+                        help='Discount factor')
+    parser.add_argument('--tau', type=float, default=0.005,  # Was 0.001
+                        help='Soft target update rate')
     parser.add_argument('--checkpoint_dir', default='./checkpoints_sac')
     parser.add_argument('--log_dir', default='./logs_sac')
 
-    # DroQ + UTD + Augmentation parameters
-    parser.add_argument('--droq_dropout', type=float, default=0.01,
+    # DroQ + UTD + Augmentation parameters - Updated for sample efficiency
+    parser.add_argument('--droq_dropout', type=float, default=0.005,  # Was 0.01
                         help='Dropout rate for DroQ (0.0 to disable)')
-    parser.add_argument('--droq_samples', type=int, default=10,
+    parser.add_argument('--droq_samples', type=int, default=2,  # Was 10 (overkill)
                         help='Number of Q-network forward passes for DroQ (M)')
-    parser.add_argument('--utd_ratio', type=int, default=4,
+    parser.add_argument('--utd_ratio', type=int, default=20,  # Was 4 (CRITICAL!)
                         help='Update-to-Data ratio (gradient steps per env step)')
-    parser.add_argument('--actor_update_freq', type=int, default=10,
+    parser.add_argument('--actor_update_freq', type=int, default=2,  # Was 10
                         help='Update actor every N critic updates')
+    parser.add_argument('--warmup_steps', type=int, default=10000,  # Was 2000
+                        help='Minimum buffer size before training starts')
     parser.add_argument('--augment_data', action='store_true',
                         help='Enable data augmentation for occupancy grids')
     parser.add_argument('--gpu_buffer', action='store_true',
diff --git a/src/tractor_bringup/tractor_bringup/convert_onnx_to_rknn.py b/src/tractor_bringup/tractor_bringup/convert_onnx_to_rknn.py
index 9c70699..6df29e6 100755
--- a/src/tractor_bringup/tractor_bringup/convert_onnx_to_rknn.py
+++ b/src/tractor_bringup/tractor_bringup/convert_onnx_to_rknn.py
@@ -33,11 +33,11 @@ import numpy as np
 
 def _load_calibration_dataset(calibration_dir: str, max_samples: int = 100):
     """Prepare calibration dataset by saving samples to .npy files and creating a dataset.txt.
-    
+
     Args:
         calibration_dir: Directory containing calibration_XXXX.npz files
         max_samples: Maximum number of samples to use
-        
+
     Returns:
         Path to the generated dataset.txt file
     """
@@ -48,43 +48,51 @@ def _load_calibration_dataset(calibration_dir: str, max_samples: int = 100):
     ])[:max_samples]
 
     print(f"Preparing calibration dataset from {len(calibration_files)} samples...")
-    
+
     # Create a temporary directory for calibration artifacts
     # We use a fixed path inside calibration_dir to avoid filling /tmp
     dataset_dir = os.path.join(calibration_dir, "rknn_dataset")
     os.makedirs(dataset_dir, exist_ok=True)
-    
+
     dataset_txt_path = os.path.join(dataset_dir, "dataset.txt")
-    
+
     valid_samples = 0
     with open(dataset_txt_path, 'w') as f:
         for i, file_path in enumerate(calibration_files):
             try:
                 data = np.load(file_path)
-                grid = data['grid']  # (64, 64) uint8
-                proprio = data['proprio']  # (7,) float32
+                grid = data['grid']  # (4, 128, 128) float32 - multi-channel occupancy
+                proprio = data['proprio']  # (10,) float32
+
+                # Validate shapes
+                if grid.shape != (4, 128, 128):
+                    print(f"⚠ Warning: Expected grid shape (4, 128, 128), got {grid.shape} in {file_path}")
+                    continue
+                if proprio.shape != (10,):
+                    print(f"⚠ Warning: Expected proprio shape (10,), got {proprio.shape} in {file_path}")
+                    continue
 
                 # Sanitize Proprio
                 proprio = np.nan_to_num(proprio, nan=0.0, posinf=100.0, neginf=-100.0)
 
-                # Grid to NCHW
-                # Grid is (H, W), add batch and channel dims -> (1, 1, 64, 64)
-                grid_nchw = grid[None, None, ...] 
-                
-                # Proprio
-                proprio_batch = proprio[None, ...] # (1, 7)
-                
+                # Add batch dimension
+                # Grid: (4, 128, 128) -> (1, 4, 128, 128)
+                grid_batch = grid[None, ...]
+
+                # Proprio: (10,) -> (1, 10)
+                proprio_batch = proprio[None, ...]
+
                 grid_path = os.path.abspath(os.path.join(dataset_dir, f"grid_{i}.npy"))
                 proprio_path = os.path.abspath(os.path.join(dataset_dir, f"proprio_{i}.npy"))
-                
-                np.save(grid_path, grid_nchw)
-                np.save(proprio_path, proprio_batch)
-                
+
+                np.save(grid_path, grid_batch.astype(np.float32))
+                np.save(proprio_path, proprio_batch.astype(np.float32))
+
                 # Write to dataset.txt (space separated)
                 f.write(f"{grid_path} {proprio_path}\n")
-                
+
                 valid_samples += 1
-                
+
             except Exception as exc:
                 print(f"⚠ Warning: Failed to process {file_path}: {exc}")
                 continue
@@ -150,12 +158,12 @@ def convert_onnx_to_rknn(
             # Disable RKNN normalization - we'll normalize in calibration generator
             # This ensures exact match between calibration and inference preprocessing
             'mean_values': [
-                [0],                 # Grid (will be pre-normalized to [0,1] in generator)
-                [0] * 11,            # Proprio (11 values: ax, ay, az, gx, gy, gz, min_depth, min_lidar, gap, prev_lin, prev_ang)
+                [0, 0, 0, 0],        # Grid: 4 channels (distance, exploration, confidence, height)
+                [0] * 10,            # Proprio (10 values: ax, ay, az, gx, gy, gz, min_depth, min_lidar, prev_lin, prev_ang)
             ],
             'std_values': [
-                [255],               # Grid: [0, 255] -> [0, 1] (std=255)
-                [1] * 11,            # Proprio (no scaling)
+                [1, 1, 1, 1],        # Grid: already normalized to [0, 1]
+                [1] * 10,            # Proprio (no scaling)
             ],
             'target_platform': target_platform,
             'optimization_level': 3
diff --git a/src/tractor_bringup/tractor_bringup/occupancy_processor.py b/src/tractor_bringup/tractor_bringup/occupancy_processor.py
index 7b1ddae..97fec65 100644
--- a/src/tractor_bringup/tractor_bringup/occupancy_processor.py
+++ b/src/tractor_bringup/tractor_bringup/occupancy_processor.py
@@ -1,5 +1,6 @@
 import numpy as np
 import cv2
+from scipy import ndimage
 
 class DepthToOccupancy:
     """
@@ -390,5 +391,486 @@ class LocalMapper:
         end_col = start_col + 64 # 160
         
         crop = self.global_map[start_row:end_row, start_col:end_col]
-        
+
         return crop.astype(np.uint8)
+
+
+class MultiChannelOccupancy:
+    """
+    Generates 4-channel 128x128 observation grid for enhanced SAC training:
+    - Channel 0: Distance to nearest obstacle [0.0, 1.0] normalized
+    - Channel 1: Exploration history [0.0, 1.0] with decay
+    - Channel 2: Obstacle confidence [0.0, 1.0] sensor reliability
+    - Channel 3: Terrain height [0.0, 1.0] normalized from ground plane
+
+    Improvements over binary 64x64 grid:
+    - 4x higher resolution (3.125 cm/pixel vs 4.69 cm/pixel)
+    - Continuous distance values preserve gradient information for Q-function
+    - Exploration history provides temporal context without LSTM
+    - Confidence channel handles sensor uncertainty
+    """
+
+    def __init__(self,
+                 grid_size=128,
+                 range_m=4.0,
+                 # Camera parameters (RealSense D435i)
+                 width=424,
+                 height=240,
+                 fx=386.0,
+                 fy=386.0,
+                 cx=212.0,
+                 cy=120.0,
+                 camera_height=0.123,
+                 camera_tilt_deg=0.0,
+                 # Thresholds
+                 obstacle_height_thresh=0.1,
+                 floor_thresh=0.08):
+
+        self.grid_size = grid_size
+        self.range_m = range_m
+        self.resolution = range_m / grid_size  # 3.125 cm/pixel for 128x128
+
+        # Camera intrinsics
+        self.width = width
+        self.height = height
+        self.fx = fx
+        self.fy = fy
+        self.cx = cx
+        self.cy = cy
+        self.camera_height = camera_height
+        self.camera_tilt = np.radians(camera_tilt_deg)
+        self.obstacle_thresh = obstacle_height_thresh
+        self.floor_thresh = floor_thresh
+
+        # Pre-compute unprojection matrices
+        u, v = np.meshgrid(np.arange(width), np.arange(height))
+        self.x_mult = (u - cx) / fx
+        self.y_mult = (v - cy) / fy
+
+        # Exploration history tracking (larger persistent map)
+        self.exploration_map = np.zeros((512, 512), dtype=np.float32)
+        self.decay_rate = 0.998  # Slower decay for larger map
+        self.exploration_center = 256  # Robot always at center
+
+        # For odometry-based stitching
+        self.last_pose = None  # (x, y, theta)
+
+    def process(self, depth_img, laser_scan, robot_pose=None):
+        """
+        Process sensor data into 4-channel observation.
+
+        Args:
+            depth_img: (H, W) numpy array, uint16 (mm) or float32 (meters)
+            laser_scan: LaserScan message or dict with 'ranges', 'angle_min', 'angle_increment'
+            robot_pose: Optional (x, y, theta) for odometry stitching
+
+        Returns:
+            obs: (4, 128, 128) float32 array, normalized to [0, 1]
+        """
+        # Channel 0: Continuous distance map
+        distance_grid = self._compute_distance_field(depth_img, laser_scan)
+
+        # Channel 1: Exploration history
+        if robot_pose is not None:
+            self._update_exploration_history(robot_pose)
+        history_crop = self._get_centered_crop(self.exploration_map, self.exploration_center, self.grid_size)
+
+        # Channel 2: Obstacle confidence
+        confidence_grid = self._compute_confidence(depth_img, laser_scan)
+
+        # Channel 3: Terrain height
+        height_grid = self._compute_terrain_height(depth_img)
+
+        # Stack channels and return
+        obs = np.stack([distance_grid, history_crop, confidence_grid, height_grid], axis=0)
+        return obs.astype(np.float32)
+
+    def _compute_distance_field(self, depth_img, laser_scan):
+        """
+        Convert depth + LiDAR to continuous distance map.
+        Returns grid where each cell contains distance to nearest obstacle.
+        Normalized to [0, 1] where 0 = obstacle at 0m, 1 = free space at range_m.
+        """
+        # Initialize with maximum distance
+        grid = np.full((self.grid_size, self.grid_size), self.range_m, dtype=np.float32)
+
+        # Process depth camera
+        if depth_img is not None and depth_img.size > 0:
+            # Handle uint16 input
+            if depth_img.dtype == np.uint16:
+                depth = depth_img.astype(np.float32) * 0.001
+            else:
+                depth = depth_img
+
+            # Unproject to 3D camera frame
+            z_c = depth
+            x_c = z_c * self.x_mult
+            y_c = z_c * self.y_mult
+
+            # Flatten for processing
+            points_c = np.stack([x_c, y_c, z_c], axis=-1).reshape(-1, 3)
+
+            # Filter valid depth
+            valid_mask = (points_c[:, 2] > 0.1) & (points_c[:, 2] < self.range_m)
+            points_c = points_c[valid_mask]
+
+            if len(points_c) > 0:
+                # Transform to rover frame
+                c = np.cos(self.camera_tilt)
+                s = np.sin(self.camera_tilt)
+
+                y_c_rot = points_c[:, 1] * c - points_c[:, 2] * s
+                z_c_rot = points_c[:, 1] * s + points_c[:, 2] * c
+                x_c_rot = points_c[:, 0]
+
+                x_r = z_c_rot
+                y_r = -x_c_rot
+                z_r = -y_c_rot + self.camera_height
+
+                # Only consider obstacles (above ground)
+                is_obstacle = z_r > self.obstacle_thresh
+
+                if np.any(is_obstacle):
+                    # Project to grid
+                    scale = self.grid_size / self.range_m
+                    grid_rows = self.grid_size - 1 - (x_r[is_obstacle] * scale).astype(np.int32)
+                    grid_cols = (self.grid_size // 2) - (y_r[is_obstacle] * scale).astype(np.int32)
+
+                    # Clip to bounds
+                    valid = (grid_rows >= 0) & (grid_rows < self.grid_size) & \
+                            (grid_cols >= 0) & (grid_cols < self.grid_size)
+
+                    grid_rows = grid_rows[valid]
+                    grid_cols = grid_cols[valid]
+                    depths = z_c_rot[is_obstacle][valid]
+
+                    # Keep minimum distance per cell
+                    for i in range(len(grid_rows)):
+                        r, c, d = grid_rows[i], grid_cols[i], depths[i]
+                        grid[r, c] = min(grid[r, c], d)
+
+        # Process LiDAR scan
+        if laser_scan is not None:
+            if hasattr(laser_scan, 'ranges'):
+                ranges = np.array(laser_scan.ranges)
+                angle_min = laser_scan.angle_min
+                angle_increment = laser_scan.angle_increment
+            else:
+                ranges = np.array(laser_scan['ranges'])
+                angle_min = laser_scan['angle_min']
+                angle_increment = laser_scan['angle_increment']
+
+            # Filter valid ranges
+            valid_mask = (ranges > 0.05) & (ranges < self.range_m)
+
+            if np.any(valid_mask):
+                # Polar to Cartesian
+                angles = angle_min + np.arange(len(ranges)) * angle_increment
+                x = ranges * np.cos(angles)
+                y = ranges * np.sin(angles)
+
+                # Apply mask
+                x = x[valid_mask]
+                y = y[valid_mask]
+                ranges = ranges[valid_mask]
+
+                # Project to grid
+                scale = self.grid_size / self.range_m
+                rows = self.grid_size - 1 - (x * scale).astype(np.int32)
+                cols = (self.grid_size // 2) - (y * scale).astype(np.int32)
+
+                # Clip to bounds
+                valid = (rows >= 0) & (rows < self.grid_size) & \
+                        (cols >= 0) & (cols < self.grid_size)
+
+                rows = rows[valid]
+                cols = cols[valid]
+                ranges = ranges[valid]
+
+                # Keep minimum distance
+                for i in range(len(rows)):
+                    r, c, d = rows[i], cols[i], ranges[i]
+                    grid[r, c] = min(grid[r, c], d)
+
+        # Apply distance transform for smooth gradients
+        # First create binary obstacle mask (obstacles within 0.5m)
+        occupied = grid < 0.5
+
+        # Compute Euclidean distance transform
+        if np.any(occupied):
+            distance_grid = ndimage.distance_transform_edt(~occupied) * self.resolution
+        else:
+            distance_grid = np.full_like(grid, self.range_m)
+
+        # Normalize to [0, 1] for network
+        distance_grid = np.clip(distance_grid / self.range_m, 0.0, 1.0)
+
+        return distance_grid
+
+    def _update_exploration_history(self, robot_pose):
+        """
+        Update persistent exploration map with current position.
+
+        Args:
+            robot_pose: (x, y, theta) in meters and radians
+        """
+        # Apply decay to entire map
+        self.exploration_map *= self.decay_rate
+
+        # Handle odometry updates
+        if self.last_pose is not None:
+            # Compute delta
+            dx = robot_pose[0] - self.last_pose[0]
+            dy = robot_pose[1] - self.last_pose[1]
+            dtheta = robot_pose[2] - self.last_pose[2]
+
+            # Shift map (inverse motion)
+            dx_px = dx / self.resolution
+            dy_px = dy / self.resolution
+
+            # Create affine transform
+            M_rot = cv2.getRotationMatrix2D(
+                (self.exploration_center, self.exploration_center),
+                -np.degrees(dtheta),
+                1.0
+            )
+            M_rot[0, 2] += dy_px  # Col shift
+            M_rot[1, 2] += dx_px  # Row shift
+
+            # Apply warp
+            self.exploration_map = cv2.warpAffine(
+                self.exploration_map,
+                M_rot,
+                (512, 512),
+                flags=cv2.INTER_LINEAR,
+                borderMode=cv2.BORDER_CONSTANT,
+                borderValue=0.0
+            )
+
+        self.last_pose = robot_pose
+
+        # Mark current position as visited
+        grid_x, grid_y = self.exploration_center, self.exploration_center
+        radius = 8  # ~25cm radius at 3.125cm/pixel
+
+        # Create circular mask
+        y, x = np.ogrid[-radius:radius+1, -radius:radius+1]
+        mask = x**2 + y**2 <= radius**2
+
+        # Mark as visited
+        y_min = max(0, grid_y - radius)
+        y_max = min(512, grid_y + radius + 1)
+        x_min = max(0, grid_x - radius)
+        x_max = min(512, grid_x + radius + 1)
+
+        mask_y_min = radius - (grid_y - y_min)
+        mask_y_max = radius + (y_max - grid_y)
+        mask_x_min = radius - (grid_x - x_min)
+        mask_x_max = radius + (x_max - grid_x)
+
+        self.exploration_map[y_min:y_max, x_min:x_max][
+            mask[mask_y_min:mask_y_max, mask_x_min:mask_x_max]
+        ] = 1.0
+
+    def _compute_confidence(self, depth_img, laser_scan):
+        """
+        Compute obstacle confidence map.
+        Higher confidence where both depth and LiDAR agree.
+        Normalized to [0, 1].
+        """
+        confidence = np.zeros((self.grid_size, self.grid_size), dtype=np.float32)
+
+        # Depth camera contributes 0.5
+        if depth_img is not None and depth_img.size > 0:
+            if depth_img.dtype == np.uint16:
+                depth = depth_img.astype(np.float32) * 0.001
+            else:
+                depth = depth_img
+
+            # Valid depth mask (simple version: non-zero)
+            valid_depth = (depth > 0.1) & (depth < self.range_m)
+
+            if np.any(valid_depth):
+                # Project valid regions to grid and mark confidence
+                z_c = depth
+                x_c = z_c * self.x_mult
+                y_c = z_c * self.y_mult
+
+                points_c = np.stack([x_c, y_c, z_c], axis=-1).reshape(-1, 3)
+                valid_mask = (points_c[:, 2] > 0.1) & (points_c[:, 2] < self.range_m)
+                points_c = points_c[valid_mask]
+
+                if len(points_c) > 0:
+                    # Transform to rover frame
+                    c = np.cos(self.camera_tilt)
+                    s = np.sin(self.camera_tilt)
+
+                    y_c_rot = points_c[:, 1] * c - points_c[:, 2] * s
+                    z_c_rot = points_c[:, 1] * s + points_c[:, 2] * c
+                    x_c_rot = points_c[:, 0]
+
+                    x_r = z_c_rot
+                    y_r = -x_c_rot
+
+                    # Project to grid
+                    scale = self.grid_size / self.range_m
+                    grid_rows = self.grid_size - 1 - (x_r * scale).astype(np.int32)
+                    grid_cols = (self.grid_size // 2) - (y_r * scale).astype(np.int32)
+
+                    valid = (grid_rows >= 0) & (grid_rows < self.grid_size) & \
+                            (grid_cols >= 0) & (grid_cols < self.grid_size)
+
+                    grid_rows = grid_rows[valid]
+                    grid_cols = grid_cols[valid]
+
+                    # Mark with confidence 0.5
+                    confidence[grid_rows, grid_cols] = 0.5
+
+        # LiDAR contributes additional 0.5
+        if laser_scan is not None:
+            if hasattr(laser_scan, 'ranges'):
+                ranges = np.array(laser_scan.ranges)
+                angle_min = laser_scan.angle_min
+                angle_increment = laser_scan.angle_increment
+            else:
+                ranges = np.array(laser_scan['ranges'])
+                angle_min = laser_scan['angle_min']
+                angle_increment = laser_scan['angle_increment']
+
+            valid_mask = (ranges > 0.05) & (ranges < self.range_m)
+
+            if np.any(valid_mask):
+                angles = angle_min + np.arange(len(ranges)) * angle_increment
+                x = ranges * np.cos(angles)
+                y = ranges * np.sin(angles)
+
+                x = x[valid_mask]
+                y = y[valid_mask]
+
+                scale = self.grid_size / self.range_m
+                rows = self.grid_size - 1 - (x * scale).astype(np.int32)
+                cols = (self.grid_size // 2) - (y * scale).astype(np.int32)
+
+                valid = (rows >= 0) & (rows < self.grid_size) & \
+                        (cols >= 0) & (cols < self.grid_size)
+
+                rows = rows[valid]
+                cols = cols[valid]
+
+                # Add 0.5 confidence (max 1.0 when both sensors agree)
+                confidence[rows, cols] = np.minimum(confidence[rows, cols] + 0.5, 1.0)
+
+        return confidence
+
+    def _compute_terrain_height(self, depth_img):
+        """
+        Compute terrain height map relative to ground plane.
+        Normalized to [0, 1] where 0 = ground level, 1 = max obstacle height.
+        """
+        height_grid = np.zeros((self.grid_size, self.grid_size), dtype=np.float32)
+
+        if depth_img is None or depth_img.size == 0:
+            return height_grid
+
+        # Handle uint16 input
+        if depth_img.dtype == np.uint16:
+            depth = depth_img.astype(np.float32) * 0.001
+        else:
+            depth = depth_img
+
+        # Unproject to 3D
+        z_c = depth
+        x_c = z_c * self.x_mult
+        y_c = z_c * self.y_mult
+
+        points_c = np.stack([x_c, y_c, z_c], axis=-1).reshape(-1, 3)
+
+        # Filter valid depth
+        valid_mask = (points_c[:, 2] > 0.1) & (points_c[:, 2] < self.range_m)
+        points_c = points_c[valid_mask]
+
+        if len(points_c) == 0:
+            return height_grid
+
+        # Transform to rover frame
+        c = np.cos(self.camera_tilt)
+        s = np.sin(self.camera_tilt)
+
+        y_c_rot = points_c[:, 1] * c - points_c[:, 2] * s
+        z_c_rot = points_c[:, 1] * s + points_c[:, 2] * c
+        x_c_rot = points_c[:, 0]
+
+        x_r = z_c_rot
+        y_r = -x_c_rot
+        z_r = -y_c_rot + self.camera_height
+
+        # Project to grid
+        scale = self.grid_size / self.range_m
+        grid_rows = self.grid_size - 1 - (x_r * scale).astype(np.int32)
+        grid_cols = (self.grid_size // 2) - (y_r * scale).astype(np.int32)
+
+        # Clip to bounds
+        valid = (grid_rows >= 0) & (grid_rows < self.grid_size) & \
+                (grid_cols >= 0) & (grid_cols < self.grid_size)
+
+        grid_rows = grid_rows[valid]
+        grid_cols = grid_cols[valid]
+        heights = z_r[valid]
+
+        # Keep maximum height per cell
+        for i in range(len(grid_rows)):
+            r, c, h = grid_rows[i], grid_cols[i], heights[i]
+            height_grid[r, c] = max(height_grid[r, c], h)
+
+        # Normalize to [0, 1] range
+        # Floor level = 0, max obstacle (0.5m) = 1.0
+        height_grid = np.clip((height_grid + 0.1) / 0.6, 0.0, 1.0)
+
+        return height_grid
+
+    def _get_centered_crop(self, map_array, center, crop_size):
+        """
+        Extract centered crop from map with robot at bottom-center.
+
+        Args:
+            map_array: Large persistent map
+            center: Robot position in map
+            crop_size: Output grid size
+
+        Returns:
+            crop: (crop_size, crop_size) array
+        """
+        # Robot should be at bottom-center of crop
+        # For 128x128 crop, robot at row 127, col 64
+        robot_row_in_crop = crop_size - 1
+        robot_col_in_crop = crop_size // 2
+
+        # Calculate map region
+        start_row = center - robot_row_in_crop
+        end_row = start_row + crop_size
+        start_col = center - robot_col_in_crop
+        end_col = start_col + crop_size
+
+        # Handle boundaries
+        if start_row < 0 or end_row > map_array.shape[0] or \
+           start_col < 0 or end_col > map_array.shape[1]:
+            # Create padded crop
+            crop = np.zeros((crop_size, crop_size), dtype=map_array.dtype)
+
+            # Calculate valid region
+            src_r_start = max(0, start_row)
+            src_r_end = min(map_array.shape[0], end_row)
+            src_c_start = max(0, start_col)
+            src_c_end = min(map_array.shape[1], end_col)
+
+            dst_r_start = src_r_start - start_row
+            dst_r_end = dst_r_start + (src_r_end - src_r_start)
+            dst_c_start = src_c_start - start_col
+            dst_c_end = dst_c_start + (src_c_end - src_c_start)
+
+            crop[dst_r_start:dst_r_end, dst_c_start:dst_c_end] = \
+                map_array[src_r_start:src_r_end, src_c_start:src_c_end]
+
+            return crop
+        else:
+            return map_array[start_row:end_row, start_col:end_col].copy()
diff --git a/src/tractor_bringup/tractor_bringup/sac_episode_runner.py b/src/tractor_bringup/tractor_bringup/sac_episode_runner.py
index 3521ab9..cc8d5e4 100644
--- a/src/tractor_bringup/tractor_bringup/sac_episode_runner.py
+++ b/src/tractor_bringup/tractor_bringup/sac_episode_runner.py
@@ -35,7 +35,7 @@ from tractor_bringup.serialization_utils import (
     serialize_status, deserialize_status
 )
 
-from tractor_bringup.occupancy_processor import DepthToOccupancy, ScanToOccupancy, LocalMapper
+from tractor_bringup.occupancy_processor import DepthToOccupancy, ScanToOccupancy, LocalMapper, MultiChannelOccupancy
 
 # ROS2 Messages
 from sensor_msgs.msg import Image, Imu, JointState, MagneticField, LaserScan
@@ -149,13 +149,17 @@ class SACEpisodeRunner(Node):
 
         # ROS2 Setup
         self.bridge = CvBridge()
-        self.occupancy_processor = DepthToOccupancy(
+        # New multi-channel occupancy processor for enhanced SAC training
+        self.occupancy_processor = MultiChannelOccupancy(
+            grid_size=128,  # Increased from 64 for better resolution
+            range_m=4.0,
             width=424, height=240,
-            camera_height=0.123, # Calculated from URDF: 0.029 + 0.08025 + 0.01375
+            camera_height=0.123,  # Calculated from URDF: 0.029 + 0.08025 + 0.01375
             camera_tilt_deg=0.0,
             obstacle_height_thresh=0.1,
             floor_thresh=0.08
         )
+        # Keep old processors for backward compatibility (can be removed later)
         self.scan_processor = ScanToOccupancy(grid_size=64, grid_range=3.0)
         self.local_mapper = LocalMapper(map_size=256, decay_rate=0.995)
         self._setup_subscribers()
@@ -304,59 +308,48 @@ class SACEpisodeRunner(Node):
         return min_dist_all, mean_side_dist, target
 
     def _compute_reward(self, action, linear_vel, angular_vel, min_lidar_dist, side_clearance, collision):
-        """LiDAR-Enhanced Reward Function."""
+        """
+        Simplified reward with 5 clear components:
+        1. Forward progress scaled by safety
+        2. Collision penalty
+        3. Exploration bonus (implicit via exploration history in observation)
+        4. Smooth control penalty
+        5. Idle penalty
+        """
         reward = 0.0
         target_speed = self._curriculum_max_speed
 
-        # 1. Forward Progress with Safety Scaling
-        # Learn to go fast ONLY when safe (min_dist > 0.5m)
-        forward_vel = max(0.0, linear_vel)
-        
-        # Safety Factor: 0.0 at 0.2m, 1.0 at 0.6m
-        safety_factor = np.clip((min_lidar_dist - 0.2) / 0.4, 0.0, 1.0)
-        
-        if forward_vel > 0.01:
-            # Reward speed, scaled by how safe it is
-            speed_reward = (forward_vel / target_speed) * safety_factor * 1.2
-            reward += speed_reward
+        # Extract metrics
+        min_dist = min_lidar_dist
+
+        # 1. Forward progress with safety scaling (primary objective)
+        safety_factor = np.clip((min_dist - 0.2) / 0.4, 0.0, 1.0)  # 0 at 0.2m, 1 at 0.6m
+        if linear_vel > 0.01:
+            speed_ratio = linear_vel / target_speed
+            reward += speed_ratio * safety_factor * 2.0  # Max +2.0
         else:
-            # Idle penalty
-            reward -= 0.3
-
-        # 2. Safety Bubble Penalty (Global)
-        # Severe penalty for getting too close to ANYTHING (approx < 25cm)
-        if min_lidar_dist < 0.25:
-            # Exponential penalty
-            prox_pen = 1.0 - (min_lidar_dist / 0.25)
-            reward -= prox_pen * 0.8
-            
-        # 3. Backward Penalty
-        if linear_vel < -0.01:
-            reward -= abs(linear_vel / target_speed) * 0.8
+            reward -= 0.5  # Idle penalty
 
-        # 4. Collision
+        # 2. Collision penalty (terminal signal)
         if collision or self._safety_override:
-            reward -= 2.0 
+            reward -= 5.0
+            return np.clip(reward, -5.0, 5.0)  # Early return
 
-        # 5. Alignment / Gap Following
-        # Reward pointing towards empty space
-        # alignment_error = abs(action[1] - self._target_heading) ... handled in main loop?
-        # Let's use the stored target heading
-        alignment_error = abs(action[1] - self._target_heading)
-        if forward_vel > 0.1:
-            reward += (0.5 - alignment_error) * 0.4
+        # 3. Exploration bonus (new grid cells)
+        # This is handled by exploration_map in observation
+        # Implicit via Q-function learning: new areas → more future rewards
+
+        # 4. Smooth control (anti-jerk)
+        angular_change = abs(action[1] - self._prev_action[1])
+        if angular_change > 0.5:  # Large steering jerk
+            reward -= angular_change * 0.3
 
-        # 6. Smoothness & Stability
-        # Penalize jerky turns
-        if abs(angular_vel) > 0.5:
-             reward -= abs(angular_vel) * 0.2
-             
-        # Corner Anti-Thrash:
-        # If in tight space (low side clearance), punish rotation heavily
-        if side_clearance < 0.4 and abs(angular_vel) > 0.2:
-            reward -= abs(angular_vel) * 1.0 # Stop spinning in narrow corridors!
+        # 5. Proximity penalty (gradual, not cliff)
+        if min_dist < 0.3:
+            proximity_penalty = (0.3 - min_dist) / 0.3  # 0 to 1
+            reward -= proximity_penalty * 1.0
 
-        return np.clip(reward, -1.0, 1.0)
+        return np.clip(reward, -5.0, 5.0)
 
     def _compute_reward_old(self, action, linear_vel, angular_vel, clearance, collision):
         """Aggressive reward function that DEMANDS forward movement.
@@ -450,113 +443,59 @@ class SACEpisodeRunner(Node):
             return
 
         # 1. Prepare Inputs
-        # Process Depth -> Occupancy Grid
+        # Process Depth + LiDAR -> Multi-Channel Occupancy Grid
         t0 = time.time()
+
+        # Get robot pose for exploration history
+        robot_pose = None
+        if self._latest_odom:
+            x, y, _, yaw = self._latest_odom
+            robot_pose = (x, y, yaw)
+
+        # NEW: Use MultiChannelOccupancy processor
+        # Returns: (4, 128, 128) float32 array, already normalized to [0, 1]
+        grid_multichannel = self.occupancy_processor.process(
+            depth_img=self._latest_depth,
+            laser_scan=self._latest_scan,
+            robot_pose=robot_pose
+        )
+
+        # Grid Input for Model: (1, 4, 128, 128)
+        grid_input = grid_multichannel[None, ...]  # Add batch dimension
+
+        # For compatibility with visualization/logging, extract channel 0 (distance)
+        # and convert back to uint8 format (0-255)
+        grid_for_viz = (grid_multichannel[0] * 255).astype(np.uint8)
+        self._latest_grid = grid_for_viz 
         
-        # 1a. Depth Grid
-        grid_depth = self.occupancy_processor.process(self._latest_depth)
-        
-        # 1b. LiDAR Grid
-        grid_scan = None
-        if self._latest_scan:
-            grid_scan = self.scan_processor.process(
-                self._latest_scan.ranges, 
-                self._latest_scan.angle_min, 
-                self._latest_scan.angle_increment
-            )
-            
-        # 1c. Fusion (Max)
-        if grid_scan is not None:
-            # Fuse: 255 wins over 128 wins over 0
-            grid = np.maximum(grid_depth, grid_scan)
-        else:
-            grid = grid_depth
-            
-        self._latest_grid = grid
-        
-        if self._latest_odom and self._prev_odom_update:
-            # Calculate ODometry Delta
-            # Odom: (x, y, lin, ang)
-            px, py, _, pyaw = self._prev_odom_update
-            cx, cy, _, cyaw = self._latest_odom
-            
-            # Global delta
-            gdx = cx - px
-            gdy = cy - py
-            
-            # Rotation delta: Prefer Odom (RF2O) over IMU now, as IMU/Mag is noisy
-            # and RF2O is very accurate for relative rotation.
-            dtheta = cyaw - pyaw
-            dtheta = (dtheta + math.pi) % (2 * math.pi) - math.pi
-            
-            # Rotate into ROBOT frame (at previous timestamp)
-            # Robot was at 'pyaw'
-            # dx_r = gdx * cos(-pyaw) - gdy * sin(-pyaw)
-            # dy_r = gdx * sin(-pyaw) + gdy * cos(-pyaw)
-            c = math.cos(-pyaw)
-            s = math.sin(-pyaw)
-            
-            dx_r = gdx * c - gdy * s
-            dy_r = gdx * s + gdy * c
-            
-            self.local_mapper.update(grid, dx_r, dy_r, dtheta)
-        else:
-            # First frame or no odom
-            self.local_mapper.update(grid, 0.0, 0.0, 0.0)
-            
-        self._prev_odom_update = self._latest_odom
-        self._prev_imu_yaw = self._latest_imu_yaw
-        
-        # Get stitched input for model
-        grid_stitched = self.local_mapper.get_model_input()
-        self._latest_grid = grid_stitched # Update visualization/logging to use stitched map
-        
-        # Grid Input for Model: (1, 1, 64, 64)
-        # We send UINT8 to server (efficiency), but Model needs FLOAT (0-1)
-        # So we normalize JUST for local inference, but store/send the raw uint8
-        
-        # Local Inference (Model expects 0-1 float)
-        grid_normalized = grid_stitched.astype(np.float32) / 255.0
-        grid_input = grid_normalized[None, None, ...] 
-        
-        # Gap Following Analysis (using Grid now!)
-        
-        # Simple heuristic: Sum of (is_free) - Sum of (is_occupied * penalty)
-        free_mask = (grid == 128).astype(np.float32)
-        occ_mask = (grid == 255).astype(np.float32)
-        
-        col_scores = np.sum(free_mask, axis=0) - np.sum(occ_mask * 5, axis=0)
-        
+        # Gap Following Analysis (for heuristic warmup and reward)
+        # Use distance channel for gap finding
+        distance_channel = grid_multichannel[0]  # (128, 128) normalized [0, 1]
+
+        # Simple heuristic: find column with maximum average distance
+        # Higher values = more free space
+        col_scores = np.mean(distance_channel, axis=0)
+
         # Smooth scores
-        col_scores = np.convolve(col_scores, np.ones(5)/5, mode='same')
-        
+        col_scores = np.convolve(col_scores, np.ones(7)/7, mode='same')
+
         best_col = np.argmax(col_scores)
 
-        # Map col 0..63 to heading -1..1
-        # With corrected grid: Col 0 = LEFT, Col 63 = RIGHT, Col 32 = CENTER
+        # Map col 0..127 to heading -1..1
+        # Col 0 = LEFT, Col 127 = RIGHT, Col 64 = CENTER
         # Heading: +1.0 = turn left, -1.0 = turn right
-        # Col 0 (left) → +1.0, Col 32 (center) → 0.0, Col 63 (right) → -1.0
-        self._target_heading = (32 - best_col) / 32.0
-        
-        # Calculate min_forward_dist from grid (for reward function)
-        # Scan center strip (width ~30cm -> 6 pixels)
-        # Grid resolution: 3.0m / 64px = 0.047m/px
-        # Center col is 32. 32 +/- 3 = 29..35
-        center_strip = grid[:, 29:35]
-        # Find obstacles (255)
-        obs_rows, _ = np.where(center_strip == 255)
-        
-        if len(obs_rows) > 0:
-            # Closest obstacle is the one with largest row index (closest to bottom/robot)
-            closest_obs_row = np.max(obs_rows)
-            # Distance in pixels
-            dist_px = 63 - closest_obs_row
-            # Distance in meters
-            self._min_forward_dist = dist_px * (3.0 / 64.0)
-        else:
-            self._min_forward_dist = 3.0 # Max range
-            
-        # Proprioception (9 values: 6-axis IMU + min_dist + prev_action)
+        self._target_heading = (64 - best_col) / 64.0
+
+        # Calculate min_forward_dist from distance channel (for reward function)
+        # Scan center strip (width ~30cm -> ~10 pixels at 3.125cm/pixel)
+        # Center col is 64. 64 +/- 5 = 59..69
+        center_strip = distance_channel[:, 59:69]
+        # Find minimum distance in front (inverse of distance = obstacle proximity)
+        # Distance channel: 1.0 = far (free), 0.0 = close (occupied)
+        min_normalized_dist = np.min(center_strip)
+        # Convert back to meters (denormalize)
+        self._min_forward_dist = min_normalized_dist * 4.0  # range_m = 4.0
+
         # Get IMU data
         if self._latest_imu:
             ax, ay, az, gx, gy, gz = self._latest_imu
@@ -565,18 +504,18 @@ class SACEpisodeRunner(Node):
 
         # LiDAR Metrics for Reward
         lidar_min, lidar_sides, gap_heading = self._process_lidar_metrics(self._latest_scan)
-        
+
         # Update target heading for Gap Follower (Warmup)
-        # But ALSO usage in reward function!
         self._target_heading = gap_heading
-        
-        # Construct 11D proprio: [ax, ay, az, gx, gy, gz, min_depth, min_lidar, gap_heading, prev_lin, prev_ang]
+
+        # Construct 10D proprio: [ax, ay, az, gx, gy, gz, min_depth, min_lidar, prev_lin, prev_ang]
+        # Note: gap_heading removed - now implicit in exploration history channel
         proprio = np.array([[
-            ax, ay, az, gx, gy, gz, 
+            ax, ay, az, gx, gy, gz,
             self._min_forward_dist,     # Min Depth (Front)
             lidar_min,                  # Min LiDAR (360 Safety)
-            gap_heading,                # Gap Heading
-            self._prev_action[0], self._prev_action[1]
+            self._prev_action[0],       # Previous linear action
+            self._prev_action[1]        # Previous angular action
         ]], dtype=np.float32)
 
         # 2. Inference (RKNN)
diff --git a/src/tractor_bringup/tractor_bringup/serialization_utils.py b/src/tractor_bringup/tractor_bringup/serialization_utils.py
index ea2c161..15ef8b2 100644
--- a/src/tractor_bringup/tractor_bringup/serialization_utils.py
+++ b/src/tractor_bringup/tractor_bringup/serialization_utils.py
@@ -19,7 +19,8 @@ def serialize_batch(batch: dict) -> bytes:
 
     Args:
         batch: Dictionary containing:
-            - grid: np.array of shape (N, 64, 64) uint8
+            - grid: np.array of shape (N, 4, 128, 128) float32
+                    4 channels: distance, exploration, confidence, height
             - proprio: np.array of shape (N, 10) float32
             - actions: np.array of shape (N, 2) float32
             - rewards: np.array of shape (N,) float32
diff --git a/status.txt b/status.txt
new file mode 100644
index 0000000..b81ba00
--- /dev/null
+++ b/status.txt
@@ -0,0 +1,21 @@
+On branch main
+Your branch is up to date with 'origin/main'.
+
+Changes not staged for commit:
+  (use "git add <file>..." to update what will be committed)
+  (use "git restore <file>..." to discard changes in working directory)
+	modified:   .claude/settings.local.json
+	modified:   remote_training_server/export_to_rknn.py
+	modified:   remote_training_server/model_architectures.py
+	modified:   remote_training_server/serialization_utils.py
+	modified:   remote_training_server/v620_sac_trainer.py
+	modified:   src/tractor_bringup/tractor_bringup/convert_onnx_to_rknn.py
+	modified:   src/tractor_bringup/tractor_bringup/occupancy_processor.py
+	modified:   src/tractor_bringup/tractor_bringup/sac_episode_runner.py
+	modified:   src/tractor_bringup/tractor_bringup/serialization_utils.py
+
+Untracked files:
+  (use "git add <file>..." to include in what will be committed)
+	status.txt
+
+no changes added to commit (use "git add" and/or "git commit -a")
diff --git a/test.txt b/test.txt
new file mode 100644
index 0000000..9daeafb
--- /dev/null
+++ b/test.txt
@@ -0,0 +1 @@
+test
